{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# IMPORT MODULES\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.contrib.layers import flatten\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from keras.layers import Convolution2D, Input, Activation, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Network Diagram\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape: (60000, 28, 28)\n",
      "Y train shape: (60000,)\n",
      "X test shape: (10000, 28, 28)\n",
      "Y test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "print(\"X train shape:\", x_train.shape)\n",
    "print(\"Y train shape:\", y_train.shape)\n",
    "print(\"X test shape:\", x_test.shape)\n",
    "print(\"Y test shape:\", y_test.shape)\n",
    "\n",
    "# Reshape the data (num_sample, image_width, image_height, channel)\n",
    "# This format is what CNN layer expects the input to be in.\n",
    "num_train_sample = x_train.shape[0]\n",
    "num_test_sample = x_test.shape[0]\n",
    "image_width = x_train.shape[1]\n",
    "image_height = x_train.shape[2]\n",
    "channel = 1\n",
    "num_class = 10\n",
    "\n",
    "x_train = x_train.reshape(num_train_sample, image_width, image_height, channel)\n",
    "x_test = x_test.reshape(num_test_sample, image_width, image_height, channel)\n",
    "\n",
    "# One hot encoding\n",
    "y_train = keras.utils.to_categorical(y_train, num_class)\n",
    "y_test = keras.utils.to_categorical(y_test, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape: (60000, 28, 28)\n",
      "Y train shape: (60000, 10)\n",
      "X test shape: (10000, 28, 28)\n",
      "Y test shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Shape after one hot encoding\n",
    "print(\"X train shape:\", x_train.shape)\n",
    "print(\"Y train shape:\", y_train.shape)\n",
    "print(\"X test shape:\", x_test.shape)\n",
    "print(\"Y test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DATA EXPLORATION\n",
    "# Load a random image\n",
    "index = random.randint(0, num_train_sample)\n",
    "image = x_train[index]\n",
    "image = image.reshape((image_width, image_height))\n",
    "\n",
    "# Visualize the image\n",
    "plt.figure(figsize = (5, 5))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "print(y_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE MODEL\n",
    "from keras.models import Model\n",
    "from keras.layers.convolutional import Conv2D\n",
    "\n",
    "# Conv2D Block: returns tensor output\n",
    "def Conv2DBlock(layer_input, num_filters, kernel_size):\n",
    "    layer = Conv2D(filters=num_filters,\n",
    "                   kernel_size=(kernel_size, kernel_size),\n",
    "                   kernel_initializer=\"he_normal\", padding=\"same\")(layer_input)\n",
    "    layer = Activation(\"relu\")(layer)\n",
    "    return layer\n",
    "\n",
    "# Network Function: returns model[input, output]\n",
    "def Network(network_input, num_filters):\n",
    "    c1 = Conv2DBlock(network_input, num_filters=num_filters, kernel_size=3)\n",
    "    c2 = Conv2DBlock(c1, num_filters=num_filters * 2, kernel_size=3)\n",
    "    # Fully Connected Layer: output = (num_sample, 10)\n",
    "    f = Flatten()(c2)\n",
    "    d = Dense(num_class)(f)\n",
    "    model = Model(inputs=[network_input], outputs=[d])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:1208: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "input_img = Input((28, 28, 1), name='img')\n",
    "model = Network(input_img, num_filters=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 50176)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                501770    \n",
      "=================================================================\n",
      "Total params: 520,586\n",
      "Trainable params: 520,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"483pt\" viewBox=\"0.00 0.00 156.00 483.00\" width=\"156pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 479)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-479 152,-479 152,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139863596992832 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139863596992832</title>\n",
       "<polygon fill=\"none\" points=\"21,-438.5 21,-474.5 127,-474.5 127,-438.5 21,-438.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74\" y=\"-452.8\">img: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139863596993000 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139863596993000</title>\n",
       "<polygon fill=\"none\" points=\"12.5,-365.5 12.5,-401.5 135.5,-401.5 135.5,-365.5 12.5,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74\" y=\"-379.8\">conv2d_1: Conv2D</text>\n",
       "</g>\n",
       "<!-- 139863596992832&#45;&gt;139863596993000 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139863596992832-&gt;139863596993000</title>\n",
       "<path d=\"M74,-438.313C74,-430.289 74,-420.547 74,-411.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"77.5001,-411.529 74,-401.529 70.5001,-411.529 77.5001,-411.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139862003160680 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139862003160680</title>\n",
       "<polygon fill=\"none\" points=\"0,-292.5 0,-328.5 148,-328.5 148,-292.5 0,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74\" y=\"-306.8\">activation_1: Activation</text>\n",
       "</g>\n",
       "<!-- 139863596993000&#45;&gt;139862003160680 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139863596993000-&gt;139862003160680</title>\n",
       "<path d=\"M74,-365.313C74,-357.289 74,-347.547 74,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"77.5001,-338.529 74,-328.529 70.5001,-338.529 77.5001,-338.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139862003160008 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139862003160008</title>\n",
       "<polygon fill=\"none\" points=\"12.5,-219.5 12.5,-255.5 135.5,-255.5 135.5,-219.5 12.5,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74\" y=\"-233.8\">conv2d_2: Conv2D</text>\n",
       "</g>\n",
       "<!-- 139862003160680&#45;&gt;139862003160008 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139862003160680-&gt;139862003160008</title>\n",
       "<path d=\"M74,-292.313C74,-284.289 74,-274.547 74,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"77.5001,-265.529 74,-255.529 70.5001,-265.529 77.5001,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139862003162864 -->\n",
       "<g class=\"node\" id=\"node5\"><title>139862003162864</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 148,-182.5 148,-146.5 0,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74\" y=\"-160.8\">activation_2: Activation</text>\n",
       "</g>\n",
       "<!-- 139862003160008&#45;&gt;139862003162864 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>139862003160008-&gt;139862003162864</title>\n",
       "<path d=\"M74,-219.313C74,-211.289 74,-201.547 74,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"77.5001,-192.529 74,-182.529 70.5001,-192.529 77.5001,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139862003345784 -->\n",
       "<g class=\"node\" id=\"node6\"><title>139862003345784</title>\n",
       "<polygon fill=\"none\" points=\"19,-73.5 19,-109.5 129,-109.5 129,-73.5 19,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74\" y=\"-87.8\">flatten_1: Flatten</text>\n",
       "</g>\n",
       "<!-- 139862003162864&#45;&gt;139862003345784 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>139862003162864-&gt;139862003345784</title>\n",
       "<path d=\"M74,-146.313C74,-138.289 74,-128.547 74,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"77.5001,-119.529 74,-109.529 70.5001,-119.529 77.5001,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139861994522160 -->\n",
       "<g class=\"node\" id=\"node7\"><title>139861994522160</title>\n",
       "<polygon fill=\"none\" points=\"23,-0.5 23,-36.5 125,-36.5 125,-0.5 23,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74\" y=\"-14.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 139862003345784&#45;&gt;139861994522160 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>139862003345784-&gt;139861994522160</title>\n",
       "<path d=\"M74,-73.3129C74,-65.2895 74,-55.5475 74,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"77.5001,-46.5288 74,-36.5288 70.5001,-46.5289 77.5001,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize network\n",
    "plot_model(model, to_file='model.png')\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:1297: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "X train shape: (60000, 28, 28)\n",
      "Y train shape: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Setup model (specify optimizer, loss function and metrics)\n",
    "model.compile(optimizer = Adam(), loss = 'mean_squared_error', metrics = [\"accuracy\", \"binary_accuracy\", \"mse\"])\n",
    "\n",
    "print(\"X train shape:\", x_train.shape)\n",
    "print(\"Y train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected img to have 4 dimensions, but got array with shape (60000, 28, 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-71205c203bfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1433\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1435\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1436\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1309\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1311\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1312\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1313\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    125\u001b[0m                                  \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                                  \u001b[0;34m' dimensions, but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                                  str(array.shape))\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected img to have 4 dimensions, but got array with shape (60000, 28, 28)"
     ]
    }
   ],
   "source": [
    "results = model.fit(x_train, y_train, batch_size=16, epochs=5, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = x_train[0]\n",
    "sample = sample.reshape(1, 28, 28, 1)\n",
    "\n",
    "print(model.predict(sample))\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"03-16-2019-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
